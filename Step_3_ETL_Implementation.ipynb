{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdZTu19yv7L7qJVq3DICMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asusatijo/ds-2002-final-project/blob/main/Step_3_ETL_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    file_path_1 = \"OzoneNational.csv\"\n",
        "    file_path_2 = \"co2_annmean_gl.csv\"\n",
        "\n",
        "data_1 = pd.read_csv(file_path_1)\n",
        "data_2 = pd.read_csv(file_path_2)\n",
        "print(data_1.head())\n",
        "print(data_2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxajDTp0EuAe",
        "outputId": "8a740cc3-53f4-4086-ff76-64888fa5884f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year      Mean  Number of Trend Sites  10th Percentile  90th Percentile  \\\n",
            "0  1980  0.094925                    134            0.070            0.116   \n",
            "1  1981  0.092347                    134            0.071            0.115   \n",
            "2  1982  0.090713                    134            0.069            0.115   \n",
            "3  1983  0.098052                    134            0.071            0.122   \n",
            "4  1984  0.089019                    134            0.068            0.112   \n",
            "\n",
            "  Units  \n",
            "0   ppm  \n",
            "1   ppm  \n",
            "2   ppm  \n",
            "3   ppm  \n",
            "4   ppm  \n",
            "   year    mean   unc\n",
            "0  1979  336.85  0.11\n",
            "1  1980  338.91  0.07\n",
            "2  1981  340.11  0.09\n",
            "3  1982  340.86  0.03\n",
            "4  1983  342.53  0.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "# 1. Extract Step\n",
        "def extract_data(file_path_1, file_path_2):\n",
        "    \"\"\"\n",
        "    Extract data from two CSV files.\n",
        "    :param file_path_1: Path to the first CSV file.\n",
        "    :param file_path_2: Path to the second CSV file.\n",
        "    :return: Two DataFrames containing the extracted data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Extracting data from %s and %s\", file_path_1, file_path_2)\n",
        "        data_1 = pd.read_csv(file_path_1)\n",
        "        data_2 = pd.read_csv(file_path_2)\n",
        "        logging.info(\"Data extraction successful.\")\n",
        "        return data_1, data_2\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error during data extraction: %s\", e)\n",
        "        raise\n",
        "\n",
        "# 2. Transform Step\n",
        "def transform_data(data_1, data_2):\n",
        "    \"\"\"\n",
        "    Transform and merge data from two sources.\n",
        "    :param data_1: DataFrame from the first source.\n",
        "    :param data_2: DataFrame from the second source.\n",
        "    :return: Merged and transformed DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Transforming data...\")\n",
        "        # Drop missing values from both datasets\n",
        "        data_1_na = data_1.dropna()\n",
        "        data_2_na = data_2.dropna()\n",
        "\n",
        "        # Drop 1979 year (index 0) from co2 data\n",
        "        data_2_no1979 = data_2_na.drop(index=0)\n",
        "\n",
        "        # Transformation: Making the data uniform through lowercase and renaming columns to avoid duplicates\n",
        "        data_1_clean = data_1_na.rename(columns={\"Year\": \"year\", \"Mean\":\"mean_of_ozone\"})\n",
        "\n",
        "        # Transformation: Renaming columns for consistency\n",
        "        data_1_new = data_1_clean.rename(columns={\"year\": \"year_of_ozone_and_co2\", \"Number of Trend Sites\": \"Number_of_Trend_Sites\", \"10th Percentile\": \"10th_Percentile\", \"90th Percentile\":\"90th_Percentile\"})\n",
        "        data_2_new = data_2_no1979.rename(columns={\"year\": \"year_of_ozone_and_co2\"})\n",
        "\n",
        "        # Transformation: Merging datasets on a common column\n",
        "        merged_data = pd.merge(data_1_new, data_2_new, on=\"year_of_ozone_and_co2\", how=\"inner\")\n",
        "\n",
        "        # Transformation: Adding a calculated column\n",
        "        merged_data[\"mean_of_ozone_and_co2\"] = merged_data[\"mean_of_ozone\"] + merged_data[\"mean\"]\n",
        "\n",
        "        # Transformation: Filtering rows based on a condition\n",
        "        merged_data = merged_data[merged_data[\"mean_of_ozone_and_co2\"] > 300]\n",
        "\n",
        "        logging.info(\"Data transformation successful.\")\n",
        "        return merged_data\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error during data transformation: %s\", e)\n",
        "        raise\n",
        "\n",
        "# 3. Load Step - Save as SQL\n",
        "def load_data_to_sql(data, sql_file_path):\n",
        "    \"\"\"\n",
        "    Convert the DataFrame into SQL insert statements and save them to a .sql file.\n",
        "    :param data: DataFrame containing the transformed data.\n",
        "    :param sql_file_path: Path to the SQL file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Loading data to SQL file: %s\", sql_file_path)\n",
        "\n",
        "        # Open the file in write mode\n",
        "        with open(sql_file_path, \"w\") as sql_file:\n",
        "            # Write SQL commands for table creation\n",
        "            sql_file.write(\"CREATE TABLE IF NOT EXISTS transformed_data (\\n\")\n",
        "            for idx, col in enumerate(data.columns):\n",
        "                if idx == len(data.columns) - 1:  # If it's the last column\n",
        "                  sql_file.write(f\"  `{col}` TEXT\\n\")  # Append value for the last column\n",
        "                else:\n",
        "                  sql_file.write(f\"  `{col}` TEXT,\\n\")  # Add a comma for all other columns\n",
        "            sql_file.write(\");\\n\\n\")\n",
        "\n",
        "            # Write SQL insert commands\n",
        "            for _, row in data.iterrows():\n",
        "                sql_file.write(\"INSERT INTO transformed_data VALUES (\")\n",
        "                sql_file.write(\", \".join([f\"'{str(x)}'\" for x in row.values]))  # Insert row values\n",
        "                sql_file.write(\");\\n\")\n",
        "\n",
        "        logging.info(\"Data successfully written to SQL file.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error during data loading to SQL file: %s\", e)\n",
        "        raise\n",
        "\n",
        "# Main ETL Workflow\n",
        "def run_etl(file_path_1, file_path_2, sql_file_path):\n",
        "    \"\"\"\n",
        "    Create the ETL workflow for two data sources and save the result as an SQL file.\n",
        "    :param file_path_1: Path to the first CSV file.\n",
        "    :param file_path_2: Path to the second CSV file.\n",
        "    :param sql_file_path: Path to the SQL file to save the result.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"Starting ETL workflow...\")\n",
        "        # Extract step\n",
        "        data_1, data_2 = extract_data(file_path_1, file_path_2)\n",
        "\n",
        "        # Transform step\n",
        "        transformed_data = transform_data(data_1, data_2)\n",
        "\n",
        "        # Load step: Save data to SQL file\n",
        "        load_data_to_sql(transformed_data, sql_file_path)\n",
        "\n",
        "        logging.info(\"ETL workflow completed successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(\"ETL workflow failed: %s\", e)\n",
        "\n",
        "# Execute ETL\n",
        "if __name__ == \"__main__\":\n",
        "    # File paths for the two data sources\n",
        "    file_path_1 = \"OzoneNational.csv\"\n",
        "    file_path_2 = \"co2_annmean_gl.csv\"\n",
        "    sql_file_path = \"ETL_transformed_data.sql\"\n",
        "\n",
        "    # Run the ETL workflow\n",
        "    run_etl(file_path_1, file_path_2, sql_file_path)\n"
      ],
      "metadata": {
        "id": "S1YPGL-dzBMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}